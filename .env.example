
--- Configuraci칩n del Proveedor de LLM ---
# Define el proveedor de LLM a utilizar. Opciones: "google" (por defecto) o "local".
LLM_PROVIDER="google"
#LLM_PROVIDER="local"

--- Configuraci칩n de Google Gemini (si LLM_PROVIDER="google") ---
GEMINI_API_KEY="TU_API_KEY_DE_GEMINI"

--- Configuraci칩n de Ollama (si LLM_PROVIDER="local") ---
# La URL base donde se est치 ejecutando tu servidor de Ollama.
OLLAMA_BASE_URL="http://host.docker.internal:11434"

# El nombre del modelo que has descargado en Ollama (ej. "llama3", "mistral", "mixtral").
OLLAMA_MODEL="llama3"
