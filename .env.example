# =============================
# Ejemplo de archivo .env para Meli Challenge
# =============================

# GENERAL
LLM_PROVIDER="openai" # Cambiar a "ollama" para usar modelo local

# --- OPENAI CONFIG ---
OPENAI_API_KEY="tu_api_key"
OPENAI_MODEL_NAME="gpt-4.1-nano"
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"

# --- COHERE CONFIG ---
COHERE_API_KEY="tu_cohere_api_key"

# --- OLLAMA CONFIG ---
# Usar http://host.docker.internal:11434 si se ejecuta desde Docker
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="llama3"